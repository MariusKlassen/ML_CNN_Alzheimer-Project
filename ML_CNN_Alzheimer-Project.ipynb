{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tx46AA9Tc6cS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df1440d8-0ac2-405f-cca2-2d21719b6ad0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "id": "p5UIRMaKaLrW",
        "outputId": "a1680048-56bb-4519-8858-0e58d3b020c5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-dc0b63311127>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Lade das Bild und konvertiere es in ein NumPy-Array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path_full\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mimg_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3226\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3227\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3228\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "###Image Array NonDemented\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Pfad zu den Bildern\n",
        "image_path = '/content/gdrive/MyDrive/Alzheimer_s Dataset/train/NonDemented'\n",
        "\n",
        "# Anzahl der Bilder\n",
        "num_images = 2560\n",
        "\n",
        "# Größe der Bilder\n",
        "image_width = 176\n",
        "image_height = 208\n",
        "\n",
        "# Initialisiere das NumPy-Array\n",
        "image_array_ND = np.zeros((num_images, image_height, image_width), dtype=np.float32)\n",
        "\n",
        "# Lade die Bilder und fülle das NumPy-Array\n",
        "for i in range(num_images):\n",
        "    image_name = f'nonDem{i}.jpg'  # Annahme: Bilder sind als \"image_1.jpg\", \"image_2.jpg\", usw. benannt\n",
        "    image_path_full = os.path.join(image_path, image_name)\n",
        "\n",
        "    # Lade das Bild und konvertiere es in ein NumPy-Array\n",
        "    img = Image.open(image_path_full)\n",
        "    img_array = np.array(img, dtype=np.float32)\n",
        "\n",
        "    # Füge das Bild zum NumPy-Array hinzu\n",
        "    image_array_ND[i, :, :] = img_array\n",
        "\n",
        "# Überprüfe die Form des erstellten Arrays\n",
        "print(image_array_ND.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Den Array in Google Drive speichern**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XUdlSCt2qPOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the save path on Google Drive\n",
        "save_path = '/content/gdrive/MyDrive/Alzheimer_s Dataset/train/NonDemented_image_array.npy'\n",
        "\n",
        "# Save the NumPy array to Google Drive\n",
        "np.save(save_path, image_array_ND)\n",
        "\n",
        "# Confirm that the array is saved\n",
        "print(f'NumPy array saved to: {save_path}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DtfgRyop9O4",
        "outputId": "3ae0d355-cbf5-42e3-8b3c-ee70f080a462"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy array saved to: /content/gdrive/MyDrive/Alzheimer_s Dataset/train/NonDemented_image_array.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Den Array von Google Drive laden**"
      ],
      "metadata": {
        "id": "NoK_C-L8qXb3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Specify the load path on Google Drive\n",
        "load_path = '/content/gdrive/MyDrive/Alzheimer_s Dataset/train/NonDemented_image_array.npy'\n",
        "\n",
        "# Load the NumPy array from Google Drive\n",
        "image_array_ND = np.load(load_path)"
      ],
      "metadata": {
        "id": "pqCvoWSfqNXm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFb0nIMrhNRS",
        "outputId": "71b2beee-cb04-4bff-da99-55ba01a17e53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2560, 1)\n"
          ]
        }
      ],
      "source": [
        "###Target Vektor NonDemented\n",
        "import numpy as np\n",
        "\n",
        "# Anzahl der Bilder\n",
        "num_images = 2560\n",
        "\n",
        "# Erstelle einen Target-Vektor mit Nullen\n",
        "target_vector_ND = np.zeros((num_images, 1), dtype=np.float32)\n",
        "\n",
        "# Überprüfe die Form des erstellten Target-Vektors\n",
        "print(target_vector_ND.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hYK0AgWMi43v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70734ad4-46b5-46f1-a234-83e5f5456c16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1792, 208, 176)\n"
          ]
        }
      ],
      "source": [
        "###Image Array VeryMildDemented\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Pfad zu den Bildern\n",
        "image_path = '/content/gdrive/MyDrive/Alzheimer_s Dataset/train/VeryMildDemented'\n",
        "\n",
        "# Anzahl der Bilder\n",
        "num_images = 1792\n",
        "\n",
        "# Größe der Bilder\n",
        "image_width = 176\n",
        "image_height = 208\n",
        "\n",
        "# Initialisiere das NumPy-Array\n",
        "image_array_VMD = np.zeros((num_images, image_height, image_width), dtype=np.float32)\n",
        "\n",
        "# Lade die Bilder und fülle das NumPy-Array\n",
        "for i in range(num_images):\n",
        "    image_name = f'verymildDem{i}.jpg'  # Annahme: Bilder sind als \"image_1.jpg\", \"image_2.jpg\", usw. benannt\n",
        "    image_path_full = os.path.join(image_path, image_name)\n",
        "\n",
        "    # Lade das Bild und konvertiere es in ein NumPy-Array\n",
        "    img = Image.open(image_path_full)\n",
        "    img_array = np.array(img, dtype=np.float32)\n",
        "\n",
        "    # Füge das Bild zum NumPy-Array hinzu\n",
        "    image_array_VMD[i, :, :] = img_array\n",
        "\n",
        "# Überprüfe die Form des erstellten Arrays\n",
        "print(image_array_VMD.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Den Array in Google Drive speichern**"
      ],
      "metadata": {
        "id": "aW6u4z5k2nlu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the save path on Google Drive\n",
        "save_path = '/content/gdrive/MyDrive/Alzheimer_s Dataset/train/VeryMildDemented_image_array.npy'\n",
        "\n",
        "# Save the NumPy array to Google Drive\n",
        "np.save(save_path, image_array_VMD)\n",
        "\n",
        "# Confirm that the array is saved\n",
        "print(f'NumPy array saved to: {save_path}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHgxweqZrzpG",
        "outputId": "6d4d8ef7-2232-4833-8eda-8698f4f2aae3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy array saved to: /content/gdrive/MyDrive/Alzheimer_s Dataset/train/VeryMildDemented_image_array.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Den Array von Google Drive laden**"
      ],
      "metadata": {
        "id": "vZmFZYO42olS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Specify the load path on Google Drive\n",
        "load_path = '/content/gdrive/MyDrive/Alzheimer_s Dataset/train/VeryMildDemented_image_array.npy'\n",
        "\n",
        "# Load the NumPy array from Google Drive\n",
        "image_array_VMD = np.load(load_path)"
      ],
      "metadata": {
        "id": "uxPwzLN_r6zA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e--oYMs0kZHj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "213d624f-a5f5-4aa2-fe69-8ae469897a26"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1792, 1)\n"
          ]
        }
      ],
      "source": [
        "###Target Vektor VeryMildDemented\n",
        "import numpy as np\n",
        "\n",
        "# Anzahl der Bilder\n",
        "num_images = 1792\n",
        "\n",
        "# Erstelle einen Target-Vektor mit Nullen\n",
        "target_vector_VMD = np.ones((num_images, 1), dtype=np.float32)\n",
        "\n",
        "# Überprüfe die Form des erstellten Target-Vektors\n",
        "print(target_vector_VMD.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzA3qO6AlDKj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a09fa31-9621-40b4-c6b3-1c71e98c429c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(717, 208, 176)\n"
          ]
        }
      ],
      "source": [
        "###Image Array MildDemented\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Pfad zu den Bildern\n",
        "image_path = '/content/gdrive/MyDrive/Alzheimer_s Dataset/train/MildDemented'\n",
        "\n",
        "# Anzahl der Bilder\n",
        "num_images = 717\n",
        "\n",
        "# Größe der Bilder\n",
        "image_width = 176\n",
        "image_height = 208\n",
        "\n",
        "# Initialisiere das NumPy-Array\n",
        "image_array_MiD = np.zeros((num_images, image_height, image_width), dtype=np.float32)\n",
        "\n",
        "# Lade die Bilder und fülle das NumPy-Array\n",
        "for i in range(num_images):\n",
        "    image_name = f'mildDem{i}.jpg'  # Annahme: Bilder sind als \"image_1.jpg\", \"image_2.jpg\", usw. benannt\n",
        "    image_path_full = os.path.join(image_path, image_name)\n",
        "\n",
        "    # Lade das Bild und konvertiere es in ein NumPy-Array\n",
        "    img = Image.open(image_path_full)\n",
        "    img_array = np.array(img, dtype=np.float32)\n",
        "\n",
        "    # Füge das Bild zum NumPy-Array hinzu\n",
        "    image_array_MiD[i, :, :] = img_array\n",
        "\n",
        "# Überprüfe die Form des erstellten Arrays\n",
        "print(image_array_MiD.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Den Array in Google Drive speichern**"
      ],
      "metadata": {
        "id": "kNqY_Apr2sjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the save path on Google Drive\n",
        "save_path = '/content/gdrive/MyDrive/Alzheimer_s Dataset/train/MildDemented_image_array.npy'\n",
        "\n",
        "# Save the NumPy array to Google Drive\n",
        "np.save(save_path, image_array_MiD)\n",
        "\n",
        "# Confirm that the array is saved\n",
        "print(f'NumPy array saved to: {save_path}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7wpikwiwsIXp",
        "outputId": "d83c2511-3d6b-4ca9-d04d-69c0a24cc791"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy array saved to: /content/gdrive/MyDrive/Alzheimer_s Dataset/train/MildDemented_image_array.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Den Array von Google Drive laden**"
      ],
      "metadata": {
        "id": "kuB1S74u2tmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Specify the load path on Google Drive\n",
        "load_path = '/content/gdrive/MyDrive/Alzheimer_s Dataset/train/MildDemented_image_array.npy'\n",
        "\n",
        "# Load the NumPy array from Google Drive\n",
        "image_array_MiD = np.load(load_path)"
      ],
      "metadata": {
        "id": "hUnz6BDtsMXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RihYzf26lV52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4eba73a2-fdf7-4a16-a2b0-e21501bc0279"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(717, 1)\n"
          ]
        }
      ],
      "source": [
        "###Target Vektor MildDemented\n",
        "import numpy as np\n",
        "\n",
        "# Anzahl der Bilder\n",
        "num_images = 717\n",
        "\n",
        "# Erstelle einen Target-Vektor mit Nullen\n",
        "target_vector_MiD = np.ones((num_images, 1), dtype=np.float32)\n",
        "\n",
        "# Überprüfe die Form des erstellten Target-Vektors\n",
        "print(target_vector_MiD.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bdTjl0jIlbUw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89a172ef-25f0-490b-deca-55b3da4ae131"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(52, 208, 176)\n"
          ]
        }
      ],
      "source": [
        "###Image Array ModerateDemented\n",
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Pfad zu den Bildern\n",
        "image_path = '/content/gdrive/MyDrive/Alzheimer_s Dataset/train/ModerateDemented'\n",
        "\n",
        "# Anzahl der Bilder\n",
        "num_images = 52\n",
        "\n",
        "# Größe der Bilder\n",
        "image_width = 176\n",
        "image_height = 208\n",
        "\n",
        "# Initialisiere das NumPy-Array\n",
        "image_array_MoD = np.zeros((num_images, image_height, image_width), dtype=np.float32)\n",
        "\n",
        "# Lade die Bilder und fülle das NumPy-Array\n",
        "for i in range(num_images):\n",
        "    image_name = f'moderateDem{i}.jpg'  # Annahme: Bilder sind als \"image_1.jpg\", \"image_2.jpg\", usw. benannt\n",
        "    image_path_full = os.path.join(image_path, image_name)\n",
        "\n",
        "    # Lade das Bild und konvertiere es in ein NumPy-Array\n",
        "    img = Image.open(image_path_full)\n",
        "    img_array = np.array(img, dtype=np.float32)\n",
        "\n",
        "    # Füge das Bild zum NumPy-Array hinzu\n",
        "    image_array_MoD[i, :, :] = img_array\n",
        "\n",
        "# Überprüfe die Form des erstellten Arrays\n",
        "print(image_array_MoD.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Den Array in Google Drive speichern**"
      ],
      "metadata": {
        "id": "R_29snuY2wzQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the save path on Google Drive\n",
        "save_path = '/content/gdrive/MyDrive/Alzheimer_s Dataset/train/ModerateDemented_image_array.npy'\n",
        "\n",
        "# Save the NumPy array to Google Drive\n",
        "np.savey(save_path, image_array_MoD)\n",
        "\n",
        "# Confirm that the array is saved\n",
        "print(f'NumPy array saved to: {save_path}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4EoLrkKsVYx",
        "outputId": "dadf9510-87a3-49a2-db9e-016c18f81c10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NumPy array saved to: /content/gdrive/MyDrive/Alzheimer_s Dataset/train/ModerateDemented_image_array.npz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Den Array von Google Drive laden**"
      ],
      "metadata": {
        "id": "p4i-IYoh2xlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Specify the load path on Google Drive\n",
        "load_path = '/content/gdrive/MyDrive/Alzheimer_s Dataset/train/ModerateDemented_image_array.npy'\n",
        "\n",
        "# Load the NumPy array from Google Drive\n",
        "image_array_MoD = np.load(load_path)"
      ],
      "metadata": {
        "id": "ZYyq2Z05sce2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Z97c_Pnljge",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c0a9e6c-0ec9-4829-cc8e-c49a33cee04b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(52, 1)\n"
          ]
        }
      ],
      "source": [
        "###Target Vektor ModerateDemented\n",
        "import numpy as np\n",
        "\n",
        "# Anzahl der Bilder\n",
        "num_images = 52\n",
        "\n",
        "# Erstelle einen Target-Vektor mit Nullen\n",
        "target_vector_MoD = np.ones((num_images, 1), dtype=np.float32)\n",
        "\n",
        "# Überprüfe die Form des erstellten Target-Vektors\n",
        "print(target_vector_MoD.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AI-q6vbTo-nf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91877fba-76ef-40f0-f7fd-bde4d6e622e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5121, 208, 176)\n"
          ]
        }
      ],
      "source": [
        "### Alles Arrays zusammenführen\n",
        "\n",
        "# Führe die Arrays zusammen\n",
        "merged_array = np.concatenate((image_array_ND, image_array_VMD, image_array_MiD, image_array_MoD), axis=0)\n",
        "\n",
        "# Überprüfe die Form des zusammengeführten Arrays\n",
        "print(merged_array.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-i4_Bzlpciv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8006c889-094a-4313-ba83-59a2adc68ff1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5121, 1)\n"
          ]
        }
      ],
      "source": [
        "### Alle Target Vektoren zusammenführen\n",
        "\n",
        "# Führe die Target-Vektoren zusammen\n",
        "merged_target_vector = np.concatenate((target_vector_ND, target_vector_VMD, target_vector_MiD, target_vector_MoD), axis=0)\n",
        "\n",
        "# Überprüfe die Form des zusammengeführten Target-Vektors\n",
        "print(merged_target_vector.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0wLBFR1rCI3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8cdf550-01a0-4e43-9ea2-5dada67f2259"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainingsdaten (Bilder): (4096, 208, 176)\n",
            "Testdaten (Bilder): (1025, 208, 176)\n",
            "Trainingsdaten (Target): (4096, 1)\n",
            "Testdaten (Target): (1025, 1)\n"
          ]
        }
      ],
      "source": [
        "### Aufteilung in Trainings- und Testdaten\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Splitte die Daten in Trainings- und Testdaten\n",
        "train_images, test_images, train_targets, test_targets = train_test_split(\n",
        "    merged_array, merged_target_vector, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Überprüfe die Form der Trainings- und Testdaten\n",
        "print(\"Trainingsdaten (Bilder):\", train_images.shape)\n",
        "print(\"Testdaten (Bilder):\", test_images.shape)\n",
        "print(\"Trainingsdaten (Target):\", train_targets.shape)\n",
        "print(\"Testdaten (Target):\", test_targets.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zLufGnJwIJnb"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "\n",
        "# Erstelle ein sequentielles Modell\n",
        "model = Sequential()\n",
        "\n",
        "# Füge die Convolutional Layer hinzu\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(208, 176, 1))) #32 Kernels der Größe 3x3 #Höhe 208, Breite 176, Channels 1, weil schwarz weiß\n",
        "model.add(MaxPooling2D((2, 2))) # Pooling Layer der Größe 2x2\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "# Flattening Layer\n",
        "model.add(Flatten())\n",
        "\n",
        "# Fully Connected Layer\n",
        "num_classes = 2\n",
        "model.add(Dense(128, activation='relu')) # 128 -> Anzahl der Neurone in der fully connected Schicht. Hyperparameter.\n",
        "model.add(Dense(num_classes, activation='softmax'))  # Output Layer\n",
        "\n",
        "## Alternativer Output Layer\n",
        "#model.add(layers.Dense(128, activation='relu'))\n",
        "#model.add(layers.Dense(1, activation='sigmoid')) ##hier nur ein Neuron am Ende und mit der sigmoid Funktion binäre Klassifikation\n",
        "###Wenn Sie jedoch planen, das Modell später auf ein mehrklassiges Problem zu erweitern, könnte der zweite Ansatz mit Dense(num_classes, activation='softmax') besser skalierbar sein.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKtzcZQLIcVR"
      },
      "outputs": [],
      "source": [
        "## Modell kompilieren\n",
        "#Dieser Schritt ist notwendig, bevor man mit dem Training des Modells beginnt.\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) #oder loss binary_crossentropy für binäre Klassifikation\n",
        "#optimizer adam:  'Adam' ist ein beliebter Optimizer, der effiziente Adaptive Momentenschätzungen verwendet und oft gute Ergebnisse für viele Anwendungen liefert\n",
        "#Optimizer für die Aktualisierung der Gewichtungen der Neurone\n",
        "#loss: Verlustfunktion\n",
        "#metrics: Welche Metriken sollen während des Trainings und Evaluation des Modells überwacht werden. Hier Genauigkeit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59cAI4wfIzen",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 793
        },
        "outputId": "eaecdd4c-54c8-4745-af91-924927de71f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 5573, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (32, 1) and (32, 2) are incompatible\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-d7667f8d2dcf>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Modell trainieren\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#hier werden die Trainingsdaten in das Modell eingespeist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1151, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 5573, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (32, 1) and (32, 2) are incompatible\n"
          ]
        }
      ],
      "source": [
        "#Modell trainieren\n",
        "#hier werden die Trainingsdaten in das Modell eingespeist\n",
        "model.fit(train_images, train_targets, epochs=10, batch_size=32, validation_data=(test_images, test_targets))\n",
        "\n",
        "print(model.predict(train_images[:1]).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgbcmS7bIwl4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 759
        },
        "outputId": "00bd1d52-104f-46c3-d16d-ad0195472c20"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2066, in test_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2049, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2037, in run_step  **\n        outputs = model.test_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1919, in test_step\n        self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 5573, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 2) are incompatible\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-da182cf0b0e4>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Modell evaluieren\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#Wie gut passt das Modell auf die Testdaten?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Test Accuracy: {test_acc}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__test_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2066, in test_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2049, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 2037, in run_step  **\n        outputs = model.test_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1919, in test_step\n        self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1209, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 2221, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 5573, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 2) are incompatible\n"
          ]
        }
      ],
      "source": [
        "#Modell evaluieren\n",
        "#Wie gut passt das Modell auf die Testdaten?\n",
        "test_loss, test_acc = model.evaluate(test_images, test_targets)\n",
        "print(f'Test Accuracy: {test_acc}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mit einem Neuron in der Outputschicht"
      ],
      "metadata": {
        "id": "ZmFJirxXaZJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "\n",
        "# Erstelle ein sequentielles Modell\n",
        "model = Sequential()\n",
        "\n",
        "# Füge die Convolutional Layer hinzu\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(208, 176, 1))) #32 Kernels der Größe 3x3 #Höhe 208, Breite 176, Channels 1, weil schwarz weiß\n",
        "model.add(MaxPooling2D((2, 2))) # Pooling Layer der Größe 2x2\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "# Flattening Layer\n",
        "model.add(Flatten())\n",
        "\n",
        "# Fully Connected Layer\n",
        "num_classes = 2\n",
        "model.add(Dense(128, activation='relu')) # 128 -> Anzahl der Neurone in der fully connected Schicht. Hyperparameter.\n",
        "#model.add(Dense(num_classes, activation='sigmoid'))  # Output Layer\n",
        "\n",
        "## Alternativer Output Layer\n",
        "#model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid')) ##hier nur ein Neuron am Ende in der Outputlayer und mit der sigmoid Funktion binäre Klassifikation\n",
        "###Wenn Sie jedoch planen, das Modell später auf ein mehrklassiges Problem zu erweitern, könnte der zweite Ansatz mit Dense(num_classes, activation='softmax') besser skalierbar sein.\n",
        "\n"
      ],
      "metadata": {
        "id": "GRAde_V4aYeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Modell kompilieren\n",
        "#Dieser Schritt ist notwendig, bevor man mit dem Training des Modells beginnt.\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) #oder loss binary_crossentropy für binäre Klassifikation\n",
        "#optimizer adam:  'Adam' ist ein beliebter Optimizer, der effiziente Adaptive Momentenschätzungen verwendet und oft gute Ergebnisse für viele Anwendungen liefert\n",
        "#Optimizer für die Aktualisierung der Gewichtungen der Neurone\n",
        "#loss: Verlustfunktion\n",
        "#metrics: Welche Metriken sollen während des Trainings und Evaluation des Modells überwacht werden. Hier Genauigkeit"
      ],
      "metadata": {
        "id": "M8JDkk0PajBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Modell trainieren\n",
        "#hier werden die Trainingsdaten in das Modell eingespeist\n",
        "# Mit Sigmoid anstatt Softmax viel bessere Accuracy\n",
        "\n",
        "model.fit(train_images, train_targets, epochs=10, batch_size=32, validation_data=(test_images, test_targets))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "BKRSOrcbalQ5",
        "outputId": "cb77c2a7-3206-4fe0-fdb2-1e6c37111b25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "127/128 [============================>.] - ETA: 0s - loss: 9.4902 - accuracy: 0.6117"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-a49060ac07ab>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Mit Sigmoid anstatt Softmax viel bessere Accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_targets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1839\u001b[0m                     \u001b[0;31m# Create data_handler for evaluation and cache it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1840\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"_eval_data_handler\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1841\u001b[0;31m                         self._eval_data_handler = data_adapter.get_data_handler(\n\u001b[0m\u001b[1;32m   1842\u001b[0m                             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1843\u001b[0m                             \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorExactEvalDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute, pss_evaluation_shards)\u001b[0m\n\u001b[1;32m   1290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m         \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m         self._adapter = adapter_cls(\n\u001b[0m\u001b[1;32m   1293\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m         \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36mslice_inputs\u001b[0;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \"\"\"\n\u001b[1;32m    387\u001b[0m         dataset = tf.data.Dataset.zip(\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         )\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensors\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    738\u001b[0m     \u001b[0;31m# pylint: disable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfrom_tensors_op\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrom_tensors_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_from_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m     \u001b[0;31m# pylint: enable=g-import-not-at-top,protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/from_tensors_op.py\u001b[0m in \u001b[0;36m_from_tensors\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_from_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=unused-private-name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_TensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/from_tensors_op.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element, name)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     variant_tensor = gen_dataset_ops.tensor_dataset(\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0moutput_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_flat_tensor_shapes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_structure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mtensor_dataset\u001b[0;34m(components, output_shapes, metadata, name)\u001b[0m\n\u001b[1;32m   7627\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7628\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7629\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   7630\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TensorDataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_shapes\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7631\u001b[0m         output_shapes, \"metadata\", metadata)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.predict(train_images[:1]).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c24gYgsERwd",
        "outputId": "76c05578-c9ca-4811-dfde-1c3c7d47a112"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 97ms/step\n",
            "(1, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Modell evaluieren\n",
        "#Wie gut passt das Modell auf die Testdaten?\n",
        "test_loss, test_acc = model.evaluate(test_images, test_targets)\n",
        "print(f'Test Accuracy: {test_acc}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdLxiQhLanhS",
        "outputId": "1bb9aa90-e3cd-49bc-c069-808c869070d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33/33 [==============================] - 0s 12ms/step - loss: 0.2061 - accuracy: 0.9366\n",
            "Test Accuracy: 0.9365853667259216\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mit 70% Training, 15% Test und 15% Validation Data"
      ],
      "metadata": {
        "id": "_A-CTg3fliuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into 70% training and 30% temporary (testing + validation)\n",
        "train_images, temp_images, train_targets, temp_targets = train_test_split(\n",
        "    merged_array, merged_target_vector, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Split the temporary data into 50% testing and 50% validation\n",
        "test_images, valid_images, test_targets, valid_targets = train_test_split(\n",
        "    temp_images, temp_targets, test_size=0.5, random_state=42\n",
        ")\n",
        "\n",
        "# Check the sizes of the resulting datasets\n",
        "print(\"Training images:\", train_images.shape)\n",
        "print(\"Testing images:\", test_images.shape)\n",
        "print(\"Validation images:\", valid_images.shape)\n",
        "print(\"Training targets:\", train_targets.shape)\n",
        "print(\"Testing targets:\", test_targets.shape)\n",
        "print(\"Validation targets:\", valid_targets.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOGM8AHylqme",
        "outputId": "46ffc785-3efe-4763-dd92-d651ff0eec84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training images: (3584, 208, 176)\n",
            "Testing images: (768, 208, 176)\n",
            "Validation images: (769, 208, 176)\n",
            "Training targets: (3584, 1)\n",
            "Testing targets: (768, 1)\n",
            "Validation targets: (769, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "\n",
        "# Erstelle ein sequentielles Modell\n",
        "model = Sequential()\n",
        "\n",
        "# Füge die Convolutional Layer hinzu\n",
        "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(208, 176, 1))) #32 Kernels der Größe 3x3 #Höhe 208, Breite 176, Channels 1, weil schwarz weiß\n",
        "model.add(MaxPooling2D((2, 2))) # Pooling Layer der Größe 2x2\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "# Flattening Layer\n",
        "model.add(Flatten())\n",
        "\n",
        "# Fully Connected Layer\n",
        "num_classes = 2\n",
        "model.add(Dense(128, activation='relu')) # 128 -> Anzahl der Neurone in der fully connected Schicht. Hyperparameter.\n",
        "#model.add(Dense(num_classes, activation='sigmoid'))  # Output Layer\n",
        "\n",
        "## Alternativer Output Layer\n",
        "#model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid')) ##hier nur ein Neuron am Ende in der Outputlayer und mit der sigmoid Funktion binäre Klassifikation\n",
        "###Wenn Sie jedoch planen, das Modell später auf ein mehrklassiges Problem zu erweitern, könnte der zweite Ansatz mit Dense(num_classes, activation='softmax') besser skalierbar sein.\n",
        "\n"
      ],
      "metadata": {
        "id": "lepuH5Evl5gx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Modell kompilieren\n",
        "#Dieser Schritt ist notwendig, bevor man mit dem Training des Modells beginnt.\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) #oder loss binary_crossentropy für binäre Klassifikation\n",
        "#optimizer adam:  'Adam' ist ein beliebter Optimizer, der effiziente Adaptive Momentenschätzungen verwendet und oft gute Ergebnisse für viele Anwendungen liefert\n",
        "#Optimizer für die Aktualisierung der Gewichtungen der Neurone\n",
        "#loss: Verlustfunktion\n",
        "#metrics: Welche Metriken sollen während des Trainings und Evaluation des Modells überwacht werden. Hier Genauigkeit"
      ],
      "metadata": {
        "id": "DxIoNw7nmB3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Modell trainieren\n",
        "#hier werden die Trainingsdaten in das Modell eingespeist\n",
        "# Mit Sigmoid anstatt Softmax viel bessere Accuracy\n",
        "\n",
        "model.fit(train_images, train_targets, epochs=10, batch_size=32, validation_data=(test_images, test_targets))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RxTUP9FmH5G",
        "outputId": "53dcb148-4e6a-4a04-828c-00cb2ed1d47c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "112/112 [==============================] - 6s 38ms/step - loss: 6.9249 - accuracy: 0.5706 - val_loss: 0.6526 - val_accuracy: 0.5794\n",
            "Epoch 2/10\n",
            "112/112 [==============================] - 4s 35ms/step - loss: 0.6187 - accuracy: 0.6473 - val_loss: 0.6018 - val_accuracy: 0.7018\n",
            "Epoch 3/10\n",
            "112/112 [==============================] - 4s 32ms/step - loss: 0.5610 - accuracy: 0.7104 - val_loss: 0.5779 - val_accuracy: 0.6888\n",
            "Epoch 4/10\n",
            "112/112 [==============================] - 4s 32ms/step - loss: 0.5444 - accuracy: 0.7249 - val_loss: 0.5605 - val_accuracy: 0.7096\n",
            "Epoch 5/10\n",
            "112/112 [==============================] - 4s 34ms/step - loss: 0.5131 - accuracy: 0.7416 - val_loss: 0.4972 - val_accuracy: 0.7396\n",
            "Epoch 6/10\n",
            "112/112 [==============================] - 4s 36ms/step - loss: 0.4348 - accuracy: 0.7997 - val_loss: 0.5287 - val_accuracy: 0.7604\n",
            "Epoch 7/10\n",
            "112/112 [==============================] - 4s 33ms/step - loss: 0.3937 - accuracy: 0.8237 - val_loss: 0.4615 - val_accuracy: 0.7891\n",
            "Epoch 8/10\n",
            "112/112 [==============================] - 4s 32ms/step - loss: 0.3278 - accuracy: 0.8557 - val_loss: 0.4328 - val_accuracy: 0.7878\n",
            "Epoch 9/10\n",
            "112/112 [==============================] - 4s 34ms/step - loss: 0.2865 - accuracy: 0.8761 - val_loss: 0.5241 - val_accuracy: 0.7799\n",
            "Epoch 10/10\n",
            "112/112 [==============================] - 4s 34ms/step - loss: 0.2303 - accuracy: 0.9082 - val_loss: 0.3591 - val_accuracy: 0.8424\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7e3cb97e0eb0>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.predict(train_images[:1]).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-nMZNbHEOaY",
        "outputId": "63613acb-6f72-4f2d-b42c-637032d3ab94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 117ms/step\n",
            "(1, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Modell evaluieren\n",
        "#Wie gut passt das Modell auf die Testdaten?\n",
        "test_loss, test_acc = model.evaluate(test_images, test_targets)\n",
        "print(f'Test Accuracy: {test_acc}')\n",
        "\n",
        "# Evaluate the model on the validation set\n",
        "val_loss, val_acc = model.evaluate(valid_images, valid_targets)\n",
        "print(f'Validation Accuracy: {val_acc}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAnn2aJHmKRd",
        "outputId": "e51f66a7-ef7a-4f42-8c47-84bf924f30f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24/24 [==============================] - 0s 13ms/step - loss: 0.3591 - accuracy: 0.8424\n",
            "Test Accuracy: 0.8424479365348816\n",
            "25/25 [==============================] - 0s 10ms/step - loss: 0.4270 - accuracy: 0.8257\n",
            "Validation Accuracy: 0.8257477283477783\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Search"
      ],
      "metadata": {
        "id": "j_Z1QHd3oVP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ycAKk6juH5r",
        "outputId": "19d8b74e-0477-42b0-bd5a-a0cb4faa88bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install scikit-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lrFadRlAyY0d",
        "outputId": "78cdc6ae-f5cf-4ce0-83e2-52cac5b79afc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "wHllqPj2wNke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade tensorflow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 983
        },
        "id": "mzbiCMa5uJgU",
        "outputId": "ccfe2b47-9d26-4ef9-e0a8-f57db2385f81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.15.0.post1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.35.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.60.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Installing collected packages: tensorflow\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.15.0\n",
            "    Uninstalling tensorflow-2.15.0:\n",
            "      Successfully uninstalled tensorflow-2.15.0\n",
            "Successfully installed tensorflow-2.15.0.post1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "metadata": {
        "id": "cH5fyIyeo8vh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 361
        },
        "outputId": "3536a4fc-79d7-4619-9889-d4bb74d83db5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'keras.wrappers'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-572c5d670dad>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomizedSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscikit_learn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKerasClassifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras.wrappers'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Funktion zur Erstellung des Modells, damit es von RandomizedSearchCV verwendet werden kann\n",
        "#Jetzt definieren wir eine Funktion (create model), die ein Modell mit den\n",
        "#gegebenen Hyperparametern enthält.\n",
        "def create_model(filters1=32, filters2=64, filters3=128, dense_units=128):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(filters1, (3, 3), activation='relu', input_shape=(208, 176, 1)))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "    model.add(Conv2D(filters2, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "    model.add(Conv2D(filters3, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(dense_units, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Definiere die Hyperparameter und ihre möglichen Werte\n",
        "#diesen Hyperparameter Raum möchten wir für die random search durchsuchen.\n",
        "param_dist = {\n",
        "    'filters1': [16, 32, 64],\n",
        "    'filters2': [32, 64, 128],\n",
        "    'filters3': [64, 128, 256],\n",
        "    'dense_units': [64, 128, 256],\n",
        "}\n",
        "\n",
        "# Erstelle das KerasClassifier-Objekt\n",
        "model = KerasClassifier(build_fn=create_model, epochs=10, batch_size=32, verbose=0)\n",
        "\n",
        "\n",
        "# Führe die RandomizedSearch durch\n",
        "#param-distributions = Hyperparameter Raum, auch vorher definiert\n",
        "#n_iter = Anzahl der zufälligen Kombinatioen, die ausprobiert werden sollen\n",
        "#cv = Anzahl der Kreuzvalidierungsfolds\n",
        "#n_jobs = 1 -> hier wird die Suche nach den besten Hyperparametern auf allen CPU-Kernen parallel durchgeführt\n",
        "#verbose = 0 währen des Trainings wird kein Forschrittbalken/ keine Ausgaben angezeigt\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=model,\n",
        "    param_distributions=param_dist,\n",
        "    cv=StratifiedKFold(n_splits=3),\n",
        "    n_iter=10,  # Anzahl der zufälligen Kombinationen zu testen\n",
        "    n_jobs=-1,\n",
        "    scoring=make_scorer(accuracy_score)\n",
        ")\n",
        "random_search_result = random_search.fit(train_images, train_targets)\n",
        "\n",
        "# Gib die besten Parameter aus\n",
        "#Hier werden die besten Hyperparameter Kombinationen ausgegeben, die während der Random Search gefunden wurden\n",
        "print(\"Best Parameters: \", random_search_result.best_params_)\n",
        "print(\"Best Score: \", random_search_result.best_score_)\n"
      ],
      "metadata": {
        "id": "ChtE4cDUo9xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import make_scorer, accuracy_score\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "\n",
        "# Define the Keras model creation function\n",
        "def create_model(filters1=32, filters2=64, filters3=128, dense_units=128):\n",
        "    model = Sequential()\n",
        "    model.add(Conv2D(filters1, (3, 3), activation='relu', input_shape=(208, 176, 1)))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "    model.add(Conv2D(filters2, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "    model.add(Conv2D(filters3, (3, 3), activation='relu'))\n",
        "    model.add(MaxPooling2D((2, 2)))\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(dense_units, activation='relu'))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Define the hyperparameter search space\n",
        "param_dist = {\n",
        "    'filters1': [16, 32],\n",
        "    'filters2': [32, 64],\n",
        "    'filters3': [64, 128],\n",
        "    'dense_units': [128],\n",
        "    'batch_size': [32],\n",
        "    'epochs': [10],\n",
        "}\n",
        "\n",
        "# Create a Keras classifier\n",
        "#keras_classifier = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "\n",
        "# Create a RandomizedSearchCV instance\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=None,\n",
        "    param_distributions=param_dist,\n",
        "    cv=StratifiedKFold(n_splits=3),\n",
        "    n_iter=10,\n",
        "    n_jobs=-1,\n",
        "    scoring=make_scorer(accuracy_score)\n",
        ")\n",
        "\n",
        "# Fit the model\n",
        "random_search_result = random_search.fit(train_images, train_targets)\n",
        "\n",
        "# Print the best parameters and score\n",
        "print(\"Best Parameters: \", random_search_result.best_params_)\n",
        "print(\"Best Score: \", random_search_result.best_score_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "4OsPE2kI0A9m",
        "outputId": "d5cc7301-9f81-417b-fed3-77661250b538"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Cannot clone object 'None' (type <class 'NoneType'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' method.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-584e9acb7aab>\u001b[0m in \u001b[0;36m<cell line: 53>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mrandom_search_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# Print the best parameters and score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    786\u001b[0m         \u001b[0mn_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_n_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         \u001b[0mbase_estimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 )\n\u001b[1;32m     78\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 raise TypeError(\n\u001b[0m\u001b[1;32m     80\u001b[0m                     \u001b[0;34m\"Cannot clone object '%s' (type %s): \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                     \u001b[0;34m\"it does not seem to be a scikit-learn \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Cannot clone object 'None' (type <class 'NoneType'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' method."
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}